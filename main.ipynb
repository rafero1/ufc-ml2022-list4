{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt  # Biblioteca para gerar gráficos\n",
    "import pandas as pd\n",
    "from sklearn import metrics, model_selection\n",
    "from scipy import stats\n",
    "from scipy.spatial import distance\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Set:\n",
    "    def __init__(self, dataset, features, output):\n",
    "        self.dataset = dataset\n",
    "        self.features = features\n",
    "        self.output = output\n",
    "\n",
    "    def get_n(self):\n",
    "        return self.dataset.shape[0]\n",
    "\n",
    "    def get_x(self):\n",
    "        return self.dataset[:, self.features]\n",
    "    \n",
    "    def get_x_apply(self, func):\n",
    "        return func(self.dataset[:, self.features])\n",
    "\n",
    "    def set_x(self, new_x):\n",
    "        self.x = new_x\n",
    "\n",
    "    def get_y(self):\n",
    "        return self.dataset[:, self.output]\n",
    "\n",
    "    def get_X(self, func=None):\n",
    "        if (func):\n",
    "            return np.c_[np.ones(self.get_n()), func(self.get_x())]\n",
    "        else:\n",
    "            return np.c_[np.ones(self.get_n()), self.get_x()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_true_positive(y, y_pred):\n",
    "    return y_pred >= 1 and y >= 1\n",
    "\n",
    "def is_false_positive(y, y_pred):\n",
    "    return y_pred >= 1 and y <= 0\n",
    "\n",
    "def is_true_negative(y, y_pred):\n",
    "    return y_pred <= 0 and y <= 0\n",
    "\n",
    "def is_false_negative(y, y_pred):\n",
    "    return y_pred <= 0 and y >= 1\n",
    "\n",
    "def confusion_matrix(y, y_pred):\n",
    "    \"\"\" returns (tp, fp, tn, fn) \"\"\"\n",
    "\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    for i, pred in enumerate(y_pred):\n",
    "        tp += 1 if is_true_positive(y[i], pred) else 0\n",
    "        fp += 1 if is_false_positive(y[i], pred) else 0\n",
    "        tn += 1 if is_true_negative(y[i], pred) else 0\n",
    "        fn += 1 if is_false_negative(y[i], pred) else 0\n",
    "    return (tp, fp, tn, fn)\n",
    "\n",
    "def accuracy(y, y_pred):\n",
    "    tp, fp, tn, fn = confusion_matrix(y, y_pred)\n",
    "    return (tp + tn) / (tp + fp + tn + fn)\n",
    "\n",
    "def precision(y, y_pred):\n",
    "    tp, fp, tn, fn = confusion_matrix(y, y_pred)\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(y, y_pred):\n",
    "    tp, fp, tn, fn = confusion_matrix(y, y_pred)\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1_score(y, y_pred):\n",
    "    precision_ = precision(y, y_pred)\n",
    "    recall_ = recall(y, y_pred)\n",
    "    return 2 * (precision_ * recall_) / (precision_ + recall_)\n",
    "\n",
    "def get_metrics(n_folds):\n",
    "    return {\n",
    "        \"accuracy\": np.zeros(n_folds),\n",
    "        \"precision\": np.zeros(n_folds),\n",
    "        \"recall\": np.zeros(n_folds),\n",
    "        \"f1_score\": np.zeros(n_folds)\n",
    "    }\n",
    "\n",
    "def calculate_metrics(metrics, y_test, y_pred):\n",
    "    metrics[\"accuracy\"][i] = (accuracy(y_test, y_pred))\n",
    "    metrics[\"precision\"][i] = (precision(y_test, y_pred))\n",
    "    metrics[\"recall\"][i] = (recall(y_test, y_pred))\n",
    "    metrics[\"f1_score\"][i] = (f1_score(y_test, y_pred))\n",
    "\n",
    "def print_metrics(metrics, name, n_folds):\n",
    "    print(\"%i-fold cross validation com %s\" % (n_folds, name))\n",
    "    print(\"acurácia: %.8f +/- %.8f\" % (metrics[\"accuracy\"].mean(), metrics[\"accuracy\"].std()))\n",
    "    print(\"revocação: %.8f +/- %.8f\" % (metrics[\"precision\"].mean(), metrics[\"precision\"].std()))\n",
    "    print(\"precisão: %.8f +/- %.8f\" % (metrics[\"recall\"].mean(), metrics[\"recall\"].std()))\n",
    "    print(\"f1-score: %.8f +/- %.8f\" % (metrics[\"f1_score\"].mean(), metrics[\"f1_score\"].std()))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_split(array, k = int):\n",
    "    \"\"\"realiza o split dos dados em k-folds\"\"\"\n",
    "    shuffled_data = np.random.permutation(array)\n",
    "    folds = np.array_split(shuffled_data, k)\n",
    "    return folds\n",
    "\n",
    "def k_fold_train_test(folds):\n",
    "    \"\"\"retorna um vetor com as configurações de treino e teste definidas pelo k-fold split\n",
    "\n",
    "    returns (i, train, test)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for i, fold in enumerate(folds):\n",
    "        train = np.vstack([x for j, x in enumerate(folds) if j != i])\n",
    "        test = fold\n",
    "        results.append((i, train, test))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questão 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere o conjunto de dados disponível em **concrete.csv**, organizado em 9 colunas, sendo as 8 primeiras colunas os atributos e a última coluna a saída. Os 8 atributos referem-se à caracterização de diferentes tipos de concreto para construção civil. A saída é a resistência à compressão do concreto (em megapascals, MPa). Maiores detalhes sobre os dados podem ser conferidos em https://www.openml.org/d/4353."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('concrete.csv', delimiter=',')\n",
    "np.random.seed(666)\n",
    "features = np.arange(8)\n",
    "output = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Considere um modelo de regressão não linear baseado em redes neurais artificiais. Separe os dados aleatoriamente em treino, validação e teste (por exemplo, 60%, 20% e 20%). Nesse cenário, treine e avalie o modelo abaixo:\n",
    "\n",
    "- **MLP (multilayer perceptron)**: 1 camada oculta e treinamento em *minibatch* via gradiente descendente estocástico com termo de *momentum*. Utilize o conjunto de validação para a justar os hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206, 9)\n",
      "(206,)\n",
      "(206, 618)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rflav\\AppData\\Local\\Temp/ipykernel_19384/336640175.py:7: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Set(data, features, output)\n",
    "train_X, test_validation_X, train_y, test_validation_y = model_selection.train_test_split(data.get_X(func=stats.zscore), data.get_y(), test_size=0.4, random_state=666, shuffle=True)\n",
    "test_X, validation_X, test_y, validation_y = model_selection.train_test_split(test_validation_X, test_validation_y, test_size=0.5, random_state=666)\n",
    "\n",
    "def accuracy_score(y_true, y_pred):\n",
    "    \"\"\" Compare y_true to y_pred and return the accuracy \"\"\"\n",
    "    accuracy = np.sum(y_true == y_pred, axis=0) / len(y_true)\n",
    "    return accuracy\n",
    "\n",
    "class TanH():\n",
    "    def __call__(self, x):\n",
    "        return 2 / (1 + np.exp(-2*x)) - 1\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return 1 - np.power(self.__call__(x), 2)\n",
    "\n",
    "class ReLU():\n",
    "    def __call__(self, x):\n",
    "        return np.where(x >= 0, x, 0)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "class Loss(object):\n",
    "    def loss(self, y_true, y_pred):\n",
    "        return NotImplementedError()\n",
    "\n",
    "    def gradient(self, y, y_pred):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def acc(self, y, y_pred):\n",
    "        return 0\n",
    "\n",
    "class CrossEntropy(Loss):\n",
    "    def __init__(self): pass\n",
    "\n",
    "    def loss(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - y * np.log(p) - (1 - y) * np.log(1 - p)\n",
    "\n",
    "    def acc(self, y, p):\n",
    "        return accuracy_score(np.argmax(y, axis=1), np.argmax(p, axis=1))\n",
    "\n",
    "    def gradient(self, y, p):\n",
    "        # Avoid division by zero\n",
    "        p = np.clip(p, 1e-15, 1 - 1e-15)\n",
    "        return - (y / p) + (1 - y) / (1 - p)\n",
    "\n",
    "class Softmax():\n",
    "    def __call__(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "\n",
    "    def gradient(self, x):\n",
    "        p = self.__call__(x)\n",
    "        return p * (1 - p)\n",
    "\n",
    "class Sigmoid():\n",
    "    def __call__(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def gradient(self, x):\n",
    "        return self.__call__(x) * (1 - self.__call__(x))\n",
    "\n",
    "class MultilayerPerceptron():\n",
    "    \"\"\"Multilayer Perceptron classifier. A fully-connected neural network with one hidden layer.\n",
    "    Unrolled to display the whole forward and backward pass.\n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_hidden: int:\n",
    "        The number of processing nodes (neurons) in the hidden layer. \n",
    "    n_iterations: float\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    learning_rate: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_hidden, n_iterations=3000, learning_rate=0.01):\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_activation = Sigmoid()\n",
    "        self.output_activation = Softmax()\n",
    "        self.loss = CrossEntropy()\n",
    "\n",
    "    def _initialize_weights(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        _, n_outputs = 1, y.shape[0]\n",
    "\n",
    "        # Hidden layer\n",
    "        limit   = 1 / math.sqrt(n_features)\n",
    "        self.W  = np.random.uniform(-limit, limit, (n_features, self.n_hidden))\n",
    "        self.w0 = np.zeros((1, self.n_hidden))\n",
    "        # Output layer\n",
    "        limit   = 1 / math.sqrt(self.n_hidden)\n",
    "        self.V  = np.random.uniform(-limit, limit, (self.n_hidden, n_outputs))\n",
    "        self.v0 = np.zeros((1, n_outputs))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        self._initialize_weights(X, y)\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "\n",
    "            # ..............\n",
    "            #  Forward Pass\n",
    "            # ..............\n",
    "\n",
    "            # HIDDEN LAYER\n",
    "            hidden_input = X.dot(self.W) + self.w0\n",
    "            hidden_output = self.hidden_activation(hidden_input)\n",
    "            # OUTPUT LAYER\n",
    "            output_layer_input = hidden_output.dot(self.V) + self.v0\n",
    "            y_pred = self.output_activation(output_layer_input)\n",
    "\n",
    "            # ...............\n",
    "            #  Backward Pass\n",
    "            # ...............\n",
    "\n",
    "            # OUTPUT LAYER\n",
    "            # Grad. w.r.t input of output layer\n",
    "            grad_wrt_out_l_input = self.loss.gradient(y, y_pred) * self.output_activation.gradient(output_layer_input)\n",
    "            grad_v = hidden_output.T.dot(grad_wrt_out_l_input)\n",
    "            grad_v0 = np.sum(grad_wrt_out_l_input, axis=0, keepdims=True)\n",
    "            # HIDDEN LAYER\n",
    "            # Grad. w.r.t input of hidden layer\n",
    "            grad_wrt_hidden_l_input = grad_wrt_out_l_input.dot(self.V.T) * self.hidden_activation.gradient(hidden_input)\n",
    "            grad_w = X.T.dot(grad_wrt_hidden_l_input)\n",
    "            grad_w0 = np.sum(grad_wrt_hidden_l_input, axis=0, keepdims=True)\n",
    "\n",
    "            # Update weights (by gradient descent)\n",
    "            # Move against the gradient to minimize loss\n",
    "            self.V  -= self.learning_rate * grad_v\n",
    "            self.v0 -= self.learning_rate * grad_v0\n",
    "            self.W  -= self.learning_rate * grad_w\n",
    "            self.w0 -= self.learning_rate * grad_w0\n",
    "\n",
    "    # Use the trained model to predict labels of X\n",
    "    def predict(self, X):\n",
    "        # Forward pass:\n",
    "        hidden_input = X.dot(self.W) + self.w0\n",
    "        hidden_output = self.hidden_activation(hidden_input)\n",
    "        output_layer_input = hidden_output.dot(self.V) + self.v0\n",
    "        y_pred = self.output_activation(output_layer_input)\n",
    "        return y_pred\n",
    "\n",
    "mlp = MultilayerPerceptron(2, 100, 0.01)\n",
    "mlp.fit(train_X, train_y)\n",
    "y_pred = mlp.predict(validation_X)\n",
    "\n",
    "print(validation_X.shape)\n",
    "print(validation_y.shape)\n",
    "print(y_pred.shape)\n",
    "accuracy_score(validation_y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Apresente as curvas da função custo nos conjuntos de treinamento e validação ao longo das épocas. Reporte também para os conjuntos de treino, validação e teste as métricas abaixo:\n",
    "- **RMSE (root mean squared error)**: \n",
    "- **MAE (mean absolute error)**:\n",
    "- **MRE (mean relative error)**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere o conjunto de dados disponível em **vowel.csv**, organizado em 11 colunas, sendo as 10 primeiras colunas os atributos e a última coluna a saída. Os 10 atributos referem-se à caracterização de amostras da fala de britânicos. A saída é o fonema de vogal correspondente, dentre as 11 possibilidades. Maiores detalhes sobre os dados podem ser conferidos em https://www.openml.org/d/307."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('vowel.csv', delimiter=',')\n",
    "np.random.seed(666)\n",
    "features = np.arange(11)\n",
    "output = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Considere um mo delo de classificação não linear baseado em redes neurais artificiais. Separe os dados aleatoriamente em treino, validação e teste (por exemplo, 60%, 20% e 20%). Nesse cenário, treine e avalie o modelo abaixo:\n",
    "\n",
    "- **MLP (multilayer perceptron)**: 1 camada o culta e treinamento em *minibatch* via gradiente descendente esto cástico com termo de *momentum*. Utilize o conjunto de validação para a justar os hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Apresente as curvas da função custo nos conjuntos de treinamento e validação ao longo das épocas. Reporte também a acurácia obtida para os conjuntos de treino, validação e teste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77ab0bf75426114283fafc7207ca0245f7de4738c2866fb9aad708a7843cc047"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
